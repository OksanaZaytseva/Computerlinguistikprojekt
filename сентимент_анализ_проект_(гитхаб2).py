# -*- coding: utf-8 -*-
"""Сентимент анализ проект (гитхаб).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11KXl0h5g_g74u6nTnikJNwNXXIllhUX-
"""

#!pip install nltk
#!pip install pymystem3
!python -m spacy download ru_core_news_lg

from collections import Counter # необходимые импорты

from pymystem3 import Mystem
mystem = Mystem()

import re

import nltk

import pandas as pd

import spacy
nlp_rus = spacy.load("ru_core_news_lg")

from google.colab import drive # для подключения гугл-диска
drive.mount('/content/drive')

add_stop_ne = []

with open('/content/drive/MyDrive/stop_ru (project).txt', 'r', encoding='utf-8') as f: #использую этот список для удаления стоп-слов
    for i in f:
        add_stop_ne.append(i.strip()) #strip
print(len(add_stop_ne))
print(add_stop_ne)

"""# **Отрицательные отзывы**

"""

with open('/content/drive/MyDrive/all_reviews_bad_joined.txt', 'r', encoding = 'utf-8') as txt_bad: #encoding='cp1251'
    read_txt_bad = txt_bad.read()

len(read_txt_bad) # строка 3856815

def normalization(read_txt_bad): # препроцессинг
    res_bad = read_txt_bad.lower()
    res_bad = res_bad.replace('-- предварительный просмотр --', ' ') # удаляем мусор
    res_bad = res_bad.replace('подтвердите просмотр фильма', ' ')
    res_bad = res_bad.replace('ответьте на несколько простых вопросов о фильме для подтверждения его\n                            просмотра вами', ' ')
    res_bad = re.sub('[0-9]+', 'num', res_bad) # заменяем все числа
    res_bad = res_bad.replace('num из num', '')
    res_bad = res_bad.replace('num', '')
    res_bad = re.sub('\\n', ' ', res_bad)
    res_lem = mystem.lemmatize(res_bad)
    res = re.sub(r'\W', ' ', str(res_lem)).split() # деление на токены
    res_clean =  [usword for usword in list(res) if usword not in add_stop_ne]
    result_bad = ' '.join(res_clean)
    return result_bad

cleaned_bad = normalization(read_txt_bad) # str 2670261, list 334596

len(cleaned_bad)

nlp_rus.max_length = len(cleaned_bad) + 100 #увеличиваем длину во избежание ошибки при выделении NER

doc_bad = nlp_rus(cleaned_bad) #выделяем именованные сущности
ents=[]
for ent in doc_bad.ents:
    ents.append(ent.text)
ner_ents = set(ents) #2946
print(len(ner_ents))

ner_ents

bad_texts = [] # убираем из корпуса именованные сущности

for k in cleaned_bad.split():
    if k not in ner_ents:
        bad_texts.append(k)
len(bad_texts) #318577

bad_analyzed = mystem.analyze(' '.join(bad_texts)) # морфологический анализ
bad_analyzed

just_adj = [] # выделяем только прилагательные
for x in bad_analyzed:
    try:
        if 'analysis' in x and x['analysis'][0]['gr'].startswith('A='):
            just_adj.append(x['text'])
    except:
        pass

len(just_adj) # 47801

freq_adj = Counter(just_adj) # считаем частоту прилагательных
freq_adj

freqadj_b = freq_adj.most_common(30) # выделяем 30 самых частотных прилагательных
freqadj_b

list_adb_freq = [] # для последующего сентимент анализа

for n in freqadj_b:
  list_adb_freq.append(n[0])
list_adb_freq

freq_trigramms = Counter(nltk.trigrams(bad_texts)) # выделяем триграммы
freq_trigramms

trigrambad = freq_trigramms.most_common(20) # найдем 20 самых частотных триграмм
trigrambad

bad_trigr = [] # для последующего сентимент анализа
for t in trigrambad:
  bad_trigr.append(t[0][0]+ ' '+ t[0][1]+ ' ' + t[0][2])
print(bad_trigr)

"""**СЕНТИМЕНТ АНАЛИЗ**"""

!pip install dostoevsky # библиотека для сентимент анализа

#!pip install fasttext-wheel

!python -m dostoevsky download fasttext-social-network-model

# сентимент-анализ текстов на русском
from dostoevsky.tokenization import RegexTokenizer
from dostoevsky.models import FastTextSocialNetworkModel

tokenizer = RegexTokenizer()
model = FastTextSocialNetworkModel(tokenizer=tokenizer)

results = model.predict(bad_trigr, k=2) # сентимент анализ триграмм (отрицательные отзывы)
for tri, sentiment in zip(bad_trigr, results):
    print(tri, ':', sentiment)

results_adj = model.predict(just_adj, k=2)
most_negative = [] # # отбираем прилагательные только с отрицательной коннотацией из всего списка прилагательных (отрицательные отзывы)
for s, sentiment in zip(just_adj, results_adj):
  if 'negative' in sentiment:
    most_negative.append([s, sentiment['negative']])

most_negative0 = []
for b in most_negative:
  if b[1] > 0.6 and [b[0], b[1]] not in most_negative0: # отбираем прилагательные с показателем больше 0.6
    most_negative0.append([b[0], b[1]])
most_negative0.sort() #сортировка по алфавиту
most_negative0

forwdcldadj = [] # список для создания облака слов
for wd in most_negative0:
   forwdcldadj.append(wd[0])
print(forwdcldadj)

results4 = model.predict(list_adb_freq, k=2) # сентимент анализ униграмм - прилагательные (отрицательные отзывы)
for rslb, sentiment in zip(list_adb_freq, results4):
    print(rslb, ':', sentiment)

"""**Облако слов**"""

# Commented out IPython magic to ensure Python compatibility.
from wordcloud import WordCloud
import matplotlib.pyplot as plt
# %matplotlib inline

from PIL import Image

import numpy as np

# Превращаем картинку в маску
#mask = np.array(Image.open('dislike1.png'))
mask = np.array(Image.open('/content/drive/MyDrive/dislike1.png'))

# Генерируем облако слов
wordcloud_adjb = WordCloud(width = 2000,
                      height = 1500,
                      background_color='black',
                      colormap='Set2',
                      mask=mask).generate(', '.join(forwdcldadj))

plt.figure(figsize=(10, 10)) # Устанавливаем размер картинки
plt.imshow(wordcloud_adjb) # Что изображаем
plt.axis("off") # Без подписей на осях
plt.show() # показать изображение

wordcloud_adjb.to_file('adj_cloud.png') # сохраним картинку

"""# **Положительные отзывы**"""

with open('/content/drive/MyDrive/all_reviews_good_joined.txt', 'r', encoding = 'utf-8') as txt_good: #encoding='cp1251'
    read_txt_good = txt_good.read()

len(read_txt_good) # str 4328135, list 648682

cleaned_good = normalization(read_txt_good) # str 2990466, list 370807

len(cleaned_good)

nlp_rus.max_length = len(cleaned_good) + 100 #увеличиваем длину во избежание ошибки при выделении NER

doc_good = nlp_rus(cleaned_good) #выделяем именованные сущности

ents_g=[]
for entg in doc_good.ents:
    ents_g.append(entg.text)
ner_ents_g = set(ents_g) #3387
print(len(ner_ents_g))

ner_ents_g

good_texts = [] # убираем из корпуса именованные сущности

for e in cleaned_good.split():
    if e not in ner_ents_g:
        good_texts.append(e)
len(good_texts) #345713

good_analyzed = mystem.analyze(' '.join(good_texts)) # морфологический анализ
good_analyzed

just_adj_g = [] # выделяем только прилагательные
for z in good_analyzed:
    try:
        if 'analysis' in z and z['analysis'][0]['gr'].startswith('A='):
            just_adj_g.append(z['text'])
    except:
        pass

len(just_adj_g) #55025

freq_adj_good = Counter(just_adj_g) # считаем частоту прилагательных
freq_adj_good

freqadjg= freq_adj_good.most_common(30) # выделяем 30 самых частотных прилагательных
freqadjg

list_adg_freq = [] # для последующего сентимент анализа

for p in freqadjg:
  list_adg_freq.append(p[0])
list_adg_freq

freq_trigramms_g = Counter(nltk.trigrams(good_texts)) # выделяем триграммы
freq_trigramms_g

trigramgood = freq_trigramms_g.most_common(20) # найдем 20 самых частотных триграмм
trigramgood

good_trigr = [] # для последующего сентимент анализа
for g in trigramgood:
  good_trigr.append(g[0][0]+ ' '+ g[0][1]+ ' ' + g[0][2])
print(good_trigr)

"""Сентимент анализ"""

results1 = model.predict(good_trigr, k=2) # сентимент анализ триграмм (положительные отзывы)
for trig, sentiment in zip(good_trigr, results1):
    print(trig, ':', sentiment)

results_adj_g = model.predict(just_adj_g, k=2)
most_positive = [] # отбираем прилагательные только с положительной коннотацией из всего списка прилагательных (положительные отзывы)
for c, sentiment in zip(just_adj_g, results_adj_g):
  if 'positive' in sentiment:
    most_positive.append([c, sentiment['positive']])

most_positive0 = []
for d in most_positive:
  if d[1] > 0.9 and [d[0], d[1]] not in most_positive0: # отбираем прилагательные с показателем больше 0.9
    most_positive0.append([d[0], d[1]])
most_positive0.sort() #сортировка по алфавиту
most_positive0

forwdcldadj_g = [] # список для создания облака слов
for w in most_positive0:
   forwdcldadj_g.append(w[0])
print(forwdcldadj_g)

results2 = model.predict(list_adg_freq, k=2) # сентимент анализ униграмм - прилагательные (положительные отзывы)
for rsl, sentiment in zip(list_adg_freq, results2):
    print(rsl, ':', sentiment)

forwdcldsub_g = [] # список для создания облака слов
for j in most_pos0:
   forwdcldsub_g.append(j[0])
print(forwdcldsub_g)

"""**Облако слов**"""

# Превращаем картинку в маску
#mask = np.array(Image.open('upvote.png'))
mask = np.array(Image.open('/content/drive/MyDrive/upvote.png'))

# Генерируем облако слов
wordcloud_adjg = WordCloud(width = 2000,
                      height = 1500,
                      background_color='black',
                      colormap='Set2',
                      mask=mask).generate(', '.join(forwdcldadj_g))

plt.figure(figsize=(10, 10)) # Устанавливаем размер картинки
plt.imshow(wordcloud_adjg) # Что изображаем
plt.axis("off") # Без подписей на осях
plt.show() # показать изображение

wordcloud_adjg.to_file('adjg_cloud.png') # сохраним картинку